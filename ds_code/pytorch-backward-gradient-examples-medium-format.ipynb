{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to post: https://medium.com/@zhang_yang/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the \"gradient\" argument in Pytorch's \"backward\" function work - explained by examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is some examples for the `gradient` argument in Pytorch's `backward` function. The math of `backward(gradient)` is explained in this [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) and these threads ([thread-1](https://discuss.pytorch.org/t/gradient-argument-in-out-backward-gradient/12742), [thread-2](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), along with some examples. Those were very helpful, but I wish there were more examples on how the numbers in the example correspond to the math, to help me more easily understand. I could not find many such examples so I will make some and write them here, so that I can look back when I forget this in two weeks.\n",
    "\n",
    "In the examples, I run code in torch, write down the math, and run the math in numpy, and show that the torch result matches the math/numpy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how Pytorch [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) explains the math:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert picture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will make examples of `x` and `y=f(x)` (we omit the arrow-hats of `x` and `y` above), and manually calculate Jacobian `J`.* \n",
    "\n",
    "Pytorch tutorial goes on with the explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert picture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above basically says: if you pass `vᵀ` as the `gradient` argument, then `y.backward(gradient)` will give you not `J` but `vᵀ・J` as the result of `x.grad`.\n",
    "\n",
    "*We will make examples of `vᵀ`, calculate `vᵀ・J` in numpy, and confirm that the result is the same as `x.grad` after calling `y.backward(gradient)` where `gradient` is `vᵀ`.*\n",
    "\n",
    "All good? Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0.dev20181130\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a simple example where `x=1` and `y = x^2` are both scalar. \n",
    "\n",
    "In pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian `J`. In this case `x` and `y` are both scalar (each only has one component `x_1` and `y_1` respectively). And we have\n",
    "\n",
    "$$\n",
    "J = \\left(\\frac{\\partial y_1}{\\partial x_1}\\right) = \\left(\\frac{\\partial y}{\\partial x}\\right) = \\left(2x\\right)\n",
    "$$\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1. As a reminder, `vᵀ` is our `gradient` with value 1. We can confirm that `vᵀ・J` gives the same result as `x.grad`. All good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1]]\n",
      "vᵀ・J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([[1,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar, non-default gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value = 100.\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value `100` for `vᵀ`, and we can see `vᵀ・J` still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "vᵀ: [[100.]]\n",
      "vᵀ・J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "vᵀ = array([[gradient_value,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a more interesting example where `x=[x_1,x_2]=[1,2]` is a vector and `y=sum(x)` is a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward() \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian `J`. In this since `x` is a vector with components `x_1` and `x_2`, and `y=x_1+x_2` is a scalar. We have\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\left( \\begin{array}{c}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}\n",
    "\\end{array} \\right)\n",
    "= \n",
    "\\left( \\begin{array}{c} 1,1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[1 1]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1, 1]])\n",
    "print('J:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1, i.e., `vᵀ` has value 1. We can confirm that `vᵀ・J` gives the same result as `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1]]\n",
      "vᵀ・J: [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([[1]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is scalar, non-default gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same as above but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1. Still, `x=[x_1,x_2]=[1,2]` is a vector and `y=sum(x)` is a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "x.grad: tensor([100., 100.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "gradient_value = 100.\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value 100 for `vᵀ`, and we can see `vᵀ・J` still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[1 1]]\n",
      "vᵀ: [[100.]]\n",
      "vᵀ・J: [[100. 100.]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1, 1]])\n",
    "print('J:')\n",
    "print(J)\n",
    "\n",
    "\n",
    "vᵀ = array([[gradient_value,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at an example where both `x=[x_1,x_2]=[1,2]` and `y=3x^2` are vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([ 6., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 1.]\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian `J`. In this since `x` is a vector with components `x_1` and `x_2`, and `y=3x^2` is a vector with component `y_1=3x_1^2` and `y_2=3x_2^2`. We have\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\left( \\begin{array}{cc}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2} \\\\\n",
    "\\end{array} \\right)\n",
    "= \n",
    "\\left( \\begin{array}{cc}\n",
    "6x_1, 0 \\\\\n",
    "0, 6x_2 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[ 6.  0.]\n",
      " [ 0. 12.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
    "print('J:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, because `y` is a vector, we must pass a `gradient` argument to `backward()`. We pass `vᵀ` with the same length as `y` and has values 1. We can confirm that `vᵀ・J` gives the same result as `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1. 1.]]\n",
      "vᵀ・J: [[ 6. 12.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([gradient_value])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is vector, non-one gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same as above but pass a non-default `gradient` with the value [1, 10] to `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 12.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([  6., 120.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = 3*x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10.]\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value [1,10] for `vᵀ`, and we can see `vᵀ・J` still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[ 6.  0.]\n",
      " [ 0. 12.]]\n",
      "vᵀ: [[ 1. 10.]]\n",
      "vᵀ・J: [[  6. 120.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], [0, 6*x[1]]])\n",
    "print('J:')\n",
    "print(J)\n",
    "\n",
    "vᵀ = array([gradient_value])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is vector - another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a slightly more complex/full-fledged example where $$x=[x_1,x_2]=[1,2]$$ is a vector with 2 components and $$y=[3x_1^2, x_1^2+2x_2^3, 10x_2]$$ is a vector with 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 3., 17., 20.], grad_fn=<CopySlices>)\n",
      "x.grad: tensor([  26., 1240.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = torch.empty(3)\n",
    "y[0] = 3*x[0]**2\n",
    "y[1] = x[0]**2 + 2*x[1]**3\n",
    "y[2] = 10*x[1]\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100.,]\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian `J`. In this since `x` is a vector with components `x_1` and `x_2`, and $$y=[y_1=3x_1^2, y_2=x_1^2+2x_2^3, y_3=10x_2]$$ We have\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\left( \\begin{array}{cc}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_3}{\\partial x_1}, \\frac{\\partial y_3}{\\partial x_2} \\\\\n",
    "\\end{array} \\right)\n",
    "= \n",
    "\\left( \\begin{array}{cc}\n",
    "6x_1, 0 \\\\\n",
    "2x_1, 6x_2^2 \\\\\n",
    "0,10\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[ 6.  0.]\n",
      " [ 2. 24.]\n",
      " [ 0. 10.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[6*x[0], 0], \n",
    "           [2*x[0], 6*x[1]**2], \n",
    "           [0, 10]])\n",
    "print('J:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, because `y` is a vector, we must pass a `gradient` argument to `backward()`. We pass `vᵀ` with the same length as `y` and has values [1., 10., 100.,]. We can confirm that `vᵀ・J` gives the same result as `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[  1.  10. 100.]]\n",
      "vᵀ・J: [[  26. 1240.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([gradient_value])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra cases: broadcast/accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's more. I've not seen it elsewhere other than [here](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), but when `y` is a scalar, you can actually pass a vector as the `gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input is scalar, output is scalar, gradient is vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in our very first simple example, let `x=1` and `y = x^2`, both scalar. But instead of a scalar, we can pass a vector of arbitrary length as `gradient`. \n",
    "\n",
    "In pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2222.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100., 1000.]\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this vector gradient, `backward` accumulates gradient for `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor(2.)\n",
      "x.grad: tensor(22.)\n",
      "x.grad: tensor(222.)\n",
      "x.grad: tensor(2222.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "gradient_value = [1., 10., 100., 1000.]\n",
    "for v in gradient_value:\n",
    "    y.backward(tensor(v), retain_graph=True) \n",
    "    print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix multiplication universe, this behavior is as if `J` is broadcast to the same length of the `gradient`.\n",
    "\n",
    "As before, the Jacobian:\n",
    "$$\n",
    "J = \\left(\\frac{\\partial y_1}{\\partial x_1}\\right) = \\left(\\frac{\\partial y}{\\partial x}\\right) = \\left(2x\\right)\n",
    "$$\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "J_broadcast:\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "J_broadcast = np.repeat(J, len(gradient_value), axis=0)\n",
    "print('J_broadcast:')\n",
    "print(J_broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that `vᵀ・J_broadcast` gives the same result as `x.grad`. All good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[   1.   10.  100. 1000.]]\n",
      "vᵀ・J_broadcast: [[2222.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([gradient_value])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J_broadcast:', vᵀ@J_broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input is vector, output is scalar, gradient is vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example of broadcast/accumulate. As in our second example, let `x=[x_1,x_2]=[1,2]` is a vector and `y=sum(x)`. But instead of a scalar, we can pass a vector of arbitrary length as `gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1111., 1111.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "gradient_value = [1., 10., 100., 1000.]\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this vector gradient, `backward` accumulates gradient for `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([1., 1.])\n",
      "x.grad: tensor([11., 11.])\n",
      "x.grad: tensor([111., 111.])\n",
      "x.grad: tensor([1111., 1111.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "y = sum(x)\n",
    "gradient_value = [1., 10., 100., 1000.]\n",
    "for v in gradient_value:\n",
    "    y.backward(tensor(v), retain_graph=True) \n",
    "    print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix multiplication universe, this behavior is as if `J` is broadcast to the same length of the `gradient`.\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\left( \\begin{array}{c}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}\n",
    "\\end{array} \\right)\n",
    "= \n",
    "\\left( \\begin{array}{c} 1,1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[1 1]]\n",
      "J_broadcast:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[1, 1]])\n",
    "print('J:', J)\n",
    "\n",
    "J_broadcast = np.repeat(J, len(gradient_value), axis=0)\n",
    "print('J_broadcast:')\n",
    "print(J_broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that `vᵀ・J_broadcast` gives the same result as `x.grad`. All good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[   1.   10.  100. 1000.]]\n",
      "vᵀ・J_broadcast: [[1111. 1111.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([gradient_value])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J_broadcast:', vᵀ@J_broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope this helps. Since there's some manual calculation of gradients here, if I made mistakes, please let me know so I can correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiv1-cpu",
   "language": "python",
   "name": "fastaiv1-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
